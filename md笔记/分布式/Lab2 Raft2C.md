# MIT6.824 Lab2C

这一次先是看了看课程，又看了看实验要求，第三次实验可以说又轻松又难。轻松是我很早第一次跑通，难是极为不稳定。

先上结果吧，这应该是我最后的波纹了

![1586859120702](https://raw.githubusercontent.com/Yang6149/typora-image/master/demo/202004/14/181201-34527.png)

## 实验目标

实验要求只是简单的对数据进行持久化，以备节点宕机或重启，或增加新节点时，可以及时的恢复。根据论文中的几个持久化的点进行简单编写后，从动手到第一次跑只花了半个小时。然后只有一个测试用例没有通过，`figure8unreliable`，噩梦也从此开始。

## 遇到的问题

这一次实验的开头真滴简单，只需要对需要持久化的数据进行持久化，然后每一次重启都读取数据进行初始化。然后跑了一次，跑不过。

这里的全部问题都是性能问题

### 1. 快速回滚

最开始看论文的时候，只是很粗略的描述这里可以进行优化，但大多数情况下不必要，因为现实生产环境下，大量的错误情况很少。然后问群友要不要进行优化，群友说刚需，毕竟实验要过。

这一点的话我是在`Reply`中添加了两个参数，`MatchIndex`和`TargetTerm`,用来表示`follower`已经成功的 Match到了哪个位置。`TargetTerm`代表Match 的位置的 Term 是多少

在`leader`发送 HB 请求的时包含参数

![1586858066836](C:\Users\hasaki\AppData\Roaming\Typora\typora-user-images\1586858066836.png)

就代表 follower 又能力识别 `prevTerm`，`follower`如果发现不匹配的话，就可以自己往后回滚，直到找到一个`Index`<=`prevIndex`的一个log，然后返回false和`matchIndex`=Index以及`TargetTerm`。相对的当 leader 收到 reply 后，也会进行判断，递减`MatchIndex`直到找到一个`rf.log[index].Term`<=`reply.TargetTerm`

可以覆盖一下情况

```
follower:	4444
leader:		46666
```

```
follower:	0444
leader:		03336
```

```
follower:	04448888
leader:		066669999
```

保证每一次回滚至少覆盖一整个Term的所有log。这样就可以做到快速回滚了。

### 2. 批处理

遇到这个问题主要是，发现在 有关FAIL相关的测试中，我的 rpc 数量总是比展示的标准输出样例的rpc数量大个好几倍，甚至十几倍，想了很久看log很久也不知道问题出在哪，最后看知乎时，看到有个人也遇到了同样的问题卡了1个星期，说时批处理。

对呀，我都忘了批处理了，如果每一次发送很多 msg rpc数量自然就少了。接着就是进行修改，修改过程就不描述了，不难。

### 3. 一些细节

之前修改的代码，把一些重要的东西删除了，比如：称为 leader 要先 AppendEntry 一次，来维护 follower，防止改变状态、收到 RequestVote 要重置 timer，这些原来写好的，不知道什么时候删掉了，浪费了很多时间。

## 总结

跑了也有几十遍遍，只有那些不会retry的测试，在接到command后重新选举这种小概率时间会测试失败（测试用例不可重传、不可避免），其他应该没问题了。

Raft 终于是写完了，前后花了有20天了吧。