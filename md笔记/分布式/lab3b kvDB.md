# MIT6.824 Lab3A

这一次的实验可谓是人间地狱，lab3 相对与 lab2 自由很多，即使有资料可以查询，但大多只是想法，具体设计很难，即使是这样也是简化了一些设计，比如 snapshot 一次传输 而不是分 chunk 传输。整个 lab3 测试起来太耗时间了，跑一次测试大概花 7-8 分钟，想要跑个几百次要花很久。所以我直接就跑其中两个所有意外条件加一起的测试用例，跑了大概有100次，没出任何问题。

![1588000148783](https://raw.githubusercontent.com/Yang6149/typora-image/master/demo/202004/27/230923-892700.png)

## 实验目标

完成对 lab3a 中的优化，增加 snapshot 功能，因为我们好歹是个数据库，在内存中存储所有数据就算了，不能连 command log 也放在内存中，所以我们需要设置一定数量的大小，超过该大小之后就进行一次 snapshot，把 command log 持久化在硬盘上。这里只是把 command log 一次性传输到 follower 节点，实际应用中肯定不能这么做，如果几百G的日志文件一次性传输，这谁顶得住啊。相关优化可以后续再进行。

## 问题

首先我们需要完成一个操作就是，最简单的把当前 log 中的数据持久化，每一个节点都可以自己进行 discard log，并持久化。直到这里其实很好实现，只需要每次检查 state 的大小就行。

但我们要考虑的一点就是什么时候持久化，如果等待全部数据commit 之后才持久化的话，万一一个节点从头到尾挂掉了，连着一个星期所有节点都持久化不了。所以我的选择是只持久化 apply 之后的数据，这样就可以保证既数据保存成功，又不会因为单个节点拖慢整个服务。

1. 持久化之后的状态改变

   这里需要新增一个变量 lastIncludedIndex，如果需要分块传输的话还需要lastIncludedTerm。这里需要考虑大量的边界条件，比如判断重复，锁问题。

   在状态改变情况下，还没来得及持久化挂掉了。

   已经改变状态，下一条指令又出现，apply 顺序不同？

   因为 snapshot 顺序不同，出现互相覆盖？

2. 因为要增加 snapshot，所以需要对原来所有 log 操作进行封装，把虚拟的 index 映射到实际的 index 上。如图 index 6 的位置实际确实 index 0.

![1587996494363](https://raw.githubusercontent.com/Yang6149/typora-image/master/demo/202004/27/220814-995058.png)

3. 对之前的 Raft 层进行改动，首先是所有有关 log 的操作进行替换，再其次就是我们需要增加一个 `LastIncludedIndex`来帮助我们进行各种判断，比如上图我们要取 index 5 的Term，我们取的其实是`lastIncludedTerm` 。出现任何需要获取已经discard 掉的 log 的操作都是不合理的，所以本职上也可以帮助我们理清思路理清 Raft 的各种操作是否合法，是否需要判重或丢弃处理。

4. server 层和 Raft 层需要保证一致，在进行snapshot操作没有结束之前的一切 appendEntry操作都是不合法的，我是通过锁阻塞的方式来保证对 snapshot 进行操作的期间从头到尾不会有第二个snapshot进行打断以及不会改变状态进行 `appendEntry`，其中尤其要注意多个线程之间的通信问题，稍不注意就会发生内存泄漏或阻塞，临时申请的 channel 需要及时delete 掉或者赋空值，GC掉。

5. 这一个是我自己的问题，我对Raft 进行了小小的优化，在每一次有新 command 之后立即进行一次心跳，这样可以极大的提高单用户的短而密集的高吞吐量。单用户的 200 条指令，只有每一个指令确认 apply 之后可以get到才会发送第二条，如果不进行优化要花费30s，优化后从启动到选举到传输不到0.5s。但劣势也明显，就是多用户的情况下，会增大很多压力，毕竟每一次命令都会传输一个command，n个用户同时进行一个 command，就会发送 n^2的Entry 数量。真正决定是否使用取决与生产环境。

   ​	修改后：![1587999960261](https://raw.githubusercontent.com/Yang6149/typora-image/master/demo/202004/27/230610-861883.png)

   ​	修改前：![1588000377252](https://raw.githubusercontent.com/Yang6149/typora-image/master/demo/202004/27/231257-792228.png)

   ​	

6. 进行了上一个优化之后，我发现测试通过不了了，我裂开了，看日志看不出毛病，就新增加了大量的日志代码，然后继续跑，但是出现的概率还是太低，跑很久才会出现，日志又太大，分析不出所以然。后来发现 3A 竟然也跑不通了，又去debug 3A，现在出bug的地方可能是 3A 部分，也可能 3B 部分也有bug，毕竟 3B 部分是建立在 3A 之上的。可以确定是我优化之后出现的bug，毕竟优化之前跑几百次也没bug，但但看这点优化没毛病，只是有效计算更加密集。我通过版本回退，再添加优化代码，也能出现bug。因为这个时候已经太累想睡觉，退回到了2C完成的版本去跑2C然后睡觉，想保证至少 Raft 是绝对安全的。今早醒来后，发现 2C 也没通过。我裂开了。排查一天是因为有一点没有判断重复导致的。条件为：

   1. leader 发送{1，2}给 follower1、follower2
   2. leader 发送{1，2，3}给 follower1、follower2
   3. follower1、follower2接受到{1，2，3}
   4. leader收到follower1，2传回的确认信息，进行 commit
   5. leader挂了
   6. follower1、follower2接受到{1，2}
   7. 选举出新 leader，传入新command 并且且在 index3 的位置传入新值并commit

   出现这种条件后才会出现一致性错误。条件为：短时间大量command，rpc不稳定，节点出现 crash。 可以说错误条件很苛刻了。

   

## 总结

对我来说 lab3 的难度和 lab2 一样难，同时那个修了2天的bug给了我深刻印象，明明之前跑lab2+lab3a 加一起有上千次，都没有出现bug ，因为我加了更加严格的条件而出现bug，还是不断的进行rpc不稳定、不断reset、高吞吐情况反复跑，才能大概1个小时跑出一次。如果放在真正的生产环境中，出现一次这个bug排查可要难死。